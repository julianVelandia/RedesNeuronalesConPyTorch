{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/julianVelandia/RedesNeuronalesConPyTorch/blob/master/redes-neuronales/3_3_Proyecto_de_redes_neuronales_FULL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predicción de Precios de Casas utilizando el Conjunto de Datos de Boston Housing\n",
        "\n",
        "Este proyecto construye, entrena y evalúa un modelo de red neuronal para predecir los precios de casas utilizando el conjunto de datos de Boston Housing."
      ],
      "metadata": {
        "id": "zYHK_kJ5ya0l"
      },
      "id": "zYHK_kJ5ya0l"
    },
    {
      "id": "30003a32",
      "cell_type": "code",
      "metadata": {
        "id": "30003a32"
      },
      "execution_count": null,
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "outputs": []
    },
    {
      "id": "01c8ed5d",
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **Descripción de las Características del Conjunto de Datos Boston Housing**\n",
        "\n",
        "| Índice | Característica | Descripción                                                                 |\n",
        "|--------|----------------|-----------------------------------------------------------------------------|\n",
        "| 0      | CRIM           | Tasa de criminalidad per cápita por localidad.                               |\n",
        "| 1      | ZN             | Proporción de terreno residencial dividido en zonas para lotes de más de 25,000 pies cuadrados. |\n",
        "| 2      | INDUS          | Proporción de acres de negocios no minoristas por ciudad.                    |\n",
        "| 3      | CHAS           | Variable ficticia del Río Charles (1 si limita con el río; 0 de lo contrario).|\n",
        "| 4      | NOX            | Concentración de óxidos nítricos (partes por 10 millones).                   |\n",
        "| 5      | RM             | Número promedio de habitaciones por vivienda.                                |\n",
        "| 6      | AGE            | Proporción de unidades ocupadas por propietarios construidas antes de 1940.  |\n",
        "| 7      | DIS            | Distancias ponderadas a cinco centros de empleo en Boston.                   |\n",
        "| 8      | RAD            | Índice de accesibilidad a carreteras radiales.                               |\n",
        "| 9      | TAX            | Tasa de impuesto a la propiedad por cada $10,000.                            |\n",
        "| 10     | PTRATIO        | Proporción alumno-maestro por ciudad.                                        |\n",
        "| 11     | B              | 1000(Bk - 0.63)^2 donde Bk es la proporción de negros por ciudad.            |\n",
        "| 12     | LSTAT          | Porcentaje de estatus inferior de la población.                              |\n"
      ],
      "metadata": {
        "id": "01c8ed5d"
      }
    },
    {
      "id": "6aae9acc",
      "cell_type": "markdown",
      "source": [
        "\n",
        "En esta sección, cargamos el conjunto de datos de Boston Housing, lo dividimos en conjuntos de entrenamiento y prueba, escalamos las características para mejorar el rendimiento de la red neuronal, y convertimos los datos en tensores de PyTorch.\n"
      ],
      "metadata": {
        "id": "6aae9acc"
      }
    },
    {
      "id": "cbd08198",
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbd08198",
        "outputId": "90b7316c-c977-4fd9-c9ce-15f24f544948"
      },
      "execution_count": null,
      "source": [
        "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
        "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
        "\n",
        "print(\"Datos originales cargados desde el archivo:\\n\")\n",
        "print(raw_df.head(), \"\\n\")\n",
        "\n",
        "# Combina filas pares completas con las dos primeras columnas de las filas impares para formar X.\n",
        "X = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
        "\n",
        "# Extrae la tercera columna de las filas impares como etiquetas (y).\n",
        "y = raw_df.values[1::2, 2]\n",
        "\n",
        "\n",
        "print(\"Características (X) después de la transformación:\\n\")\n",
        "print(X[:5], \"\\n\")\n",
        "\n",
        "print(\"Etiquetas (y) después de la transformación:\\n\")\n",
        "print(y[:5], \"\\n\")\n",
        "\n",
        "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "y_tensor = torch.tensor(y, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "print(\"Características como tensores de PyTorch (X_tensor):\\n\")\n",
        "print(X_tensor[:5], \"\\n\")\n",
        "\n",
        "print(\"Etiquetas como tensores de PyTorch (y_tensor):\\n\")\n",
        "print(y_tensor[:5], \"\\n\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Datos originales cargados desde el archivo:\n",
            "\n",
            "          0      1      2    3      4      5     6       7    8      9     10\n",
            "0    0.00632  18.00   2.31  0.0  0.538  6.575  65.2  4.0900  1.0  296.0  15.3\n",
            "1  396.90000   4.98  24.00  NaN    NaN    NaN   NaN     NaN  NaN    NaN   NaN\n",
            "2    0.02731   0.00   7.07  0.0  0.469  6.421  78.9  4.9671  2.0  242.0  17.8\n",
            "3  396.90000   9.14  21.60  NaN    NaN    NaN   NaN     NaN  NaN    NaN   NaN\n",
            "4    0.02729   0.00   7.07  0.0  0.469  7.185  61.1  4.9671  2.0  242.0  17.8 \n",
            "\n",
            "Características (X) después de la transformación:\n",
            "\n",
            "[[6.3200e-03 1.8000e+01 2.3100e+00 0.0000e+00 5.3800e-01 6.5750e+00\n",
            "  6.5200e+01 4.0900e+00 1.0000e+00 2.9600e+02 1.5300e+01 3.9690e+02\n",
            "  4.9800e+00]\n",
            " [2.7310e-02 0.0000e+00 7.0700e+00 0.0000e+00 4.6900e-01 6.4210e+00\n",
            "  7.8900e+01 4.9671e+00 2.0000e+00 2.4200e+02 1.7800e+01 3.9690e+02\n",
            "  9.1400e+00]\n",
            " [2.7290e-02 0.0000e+00 7.0700e+00 0.0000e+00 4.6900e-01 7.1850e+00\n",
            "  6.1100e+01 4.9671e+00 2.0000e+00 2.4200e+02 1.7800e+01 3.9283e+02\n",
            "  4.0300e+00]\n",
            " [3.2370e-02 0.0000e+00 2.1800e+00 0.0000e+00 4.5800e-01 6.9980e+00\n",
            "  4.5800e+01 6.0622e+00 3.0000e+00 2.2200e+02 1.8700e+01 3.9463e+02\n",
            "  2.9400e+00]\n",
            " [6.9050e-02 0.0000e+00 2.1800e+00 0.0000e+00 4.5800e-01 7.1470e+00\n",
            "  5.4200e+01 6.0622e+00 3.0000e+00 2.2200e+02 1.8700e+01 3.9690e+02\n",
            "  5.3300e+00]] \n",
            "\n",
            "Etiquetas (y) después de la transformación:\n",
            "\n",
            "[24.  21.6 34.7 33.4 36.2] \n",
            "\n",
            "Características como tensores de PyTorch (X_tensor):\n",
            "\n",
            "tensor([[6.3200e-03, 1.8000e+01, 2.3100e+00, 0.0000e+00, 5.3800e-01, 6.5750e+00,\n",
            "         6.5200e+01, 4.0900e+00, 1.0000e+00, 2.9600e+02, 1.5300e+01, 3.9690e+02,\n",
            "         4.9800e+00],\n",
            "        [2.7310e-02, 0.0000e+00, 7.0700e+00, 0.0000e+00, 4.6900e-01, 6.4210e+00,\n",
            "         7.8900e+01, 4.9671e+00, 2.0000e+00, 2.4200e+02, 1.7800e+01, 3.9690e+02,\n",
            "         9.1400e+00],\n",
            "        [2.7290e-02, 0.0000e+00, 7.0700e+00, 0.0000e+00, 4.6900e-01, 7.1850e+00,\n",
            "         6.1100e+01, 4.9671e+00, 2.0000e+00, 2.4200e+02, 1.7800e+01, 3.9283e+02,\n",
            "         4.0300e+00],\n",
            "        [3.2370e-02, 0.0000e+00, 2.1800e+00, 0.0000e+00, 4.5800e-01, 6.9980e+00,\n",
            "         4.5800e+01, 6.0622e+00, 3.0000e+00, 2.2200e+02, 1.8700e+01, 3.9463e+02,\n",
            "         2.9400e+00],\n",
            "        [6.9050e-02, 0.0000e+00, 2.1800e+00, 0.0000e+00, 4.5800e-01, 7.1470e+00,\n",
            "         5.4200e+01, 6.0622e+00, 3.0000e+00, 2.2200e+02, 1.8700e+01, 3.9690e+02,\n",
            "         5.3300e+00]]) \n",
            "\n",
            "Etiquetas como tensores de PyTorch (y_tensor):\n",
            "\n",
            "tensor([[24.0000],\n",
            "        [21.6000],\n",
            "        [34.7000],\n",
            "        [33.4000],\n",
            "        [36.2000]]) \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_test_split(X, y, test_size=0.2):\n",
        "    \"\"\"\n",
        "    Divide los datos en conjuntos de entrenamiento y prueba.\n",
        "    \"\"\"\n",
        "    indices = torch.randperm(X.size(0))\n",
        "    split = int(test_size * X.size(0))\n",
        "    train_indices, test_indices = indices[split:], indices[:split]\n",
        "\n",
        "    train_indices = indices[split:]\n",
        "    test_indices = indices[:split]\n",
        "\n",
        "    return X[train_indices], X[test_indices], y[train_indices], y[test_indices]\n",
        "\n",
        "X_train_tensor, X_test_tensor, y_train_tensor, y_test_tensor = train_test_split(X_tensor, y_tensor)\n",
        "\n",
        "print(\"Conjunto de entrenamiento (X_train_tensor):\\n\")\n",
        "print(X_train_tensor[:5], \"\\n\")\n",
        "print(\"Conjunto de prueba (X_test_tensor):\\n\")\n",
        "print(X_test_tensor[:5], \"\\n\")\n",
        "print(\"Etiquetas de entrenamiento (y_train_tensor):\\n\")\n",
        "print(y_train_tensor[:5], \"\\n\")\n",
        "print(\"Etiquetas de prueba (y_test_tensor):\\n\")\n",
        "print(y_test_tensor[:5], \"\\n\")\n",
        "\n",
        "# normalización\n",
        "mean = X_train_tensor.mean(dim=0, keepdim=True)\n",
        "std = X_train_tensor.std(dim=0, keepdim=True)\n",
        "\n",
        "X_train_tensor = (X_train_tensor - mean) / std\n",
        "X_test_tensor = (X_test_tensor - mean) / std\n",
        "\n",
        "print(\"Conjunto de entrenamiento escalado (X_train_tensor):\\n\")\n",
        "print(X_train_tensor[:5], \"\\n\")\n",
        "print(\"Conjunto de prueba escalado (X_test_tensor):\\n\")\n",
        "print(X_test_tensor[:5], \"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZdMtUfA4z6lD",
        "outputId": "05281585-b4f1-4ae5-fee4-2c233983401a"
      },
      "id": "ZdMtUfA4z6lD",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Conjunto de entrenamiento (X_train_tensor):\n",
            "\n",
            "tensor([[3.7050e-02, 2.0000e+01, 3.3300e+00, 0.0000e+00, 4.4290e-01, 6.9680e+00,\n",
            "         3.7200e+01, 5.2447e+00, 5.0000e+00, 2.1600e+02, 1.4900e+01, 3.9223e+02,\n",
            "         4.5900e+00],\n",
            "        [6.1290e-02, 2.0000e+01, 3.3300e+00, 1.0000e+00, 4.4290e-01, 7.6450e+00,\n",
            "         4.9700e+01, 5.2119e+00, 5.0000e+00, 2.1600e+02, 1.4900e+01, 3.7707e+02,\n",
            "         3.0100e+00],\n",
            "        [1.3075e+01, 0.0000e+00, 1.8100e+01, 0.0000e+00, 5.8000e-01, 5.7130e+00,\n",
            "         5.6700e+01, 2.8237e+00, 2.4000e+01, 6.6600e+02, 2.0200e+01, 3.9690e+02,\n",
            "         1.4760e+01],\n",
            "        [1.7446e-01, 0.0000e+00, 1.0590e+01, 1.0000e+00, 4.8900e-01, 5.9600e+00,\n",
            "         9.2100e+01, 3.8771e+00, 4.0000e+00, 2.7700e+02, 1.8600e+01, 3.9325e+02,\n",
            "         1.7270e+01],\n",
            "        [1.6128e+00, 0.0000e+00, 8.1400e+00, 0.0000e+00, 5.3800e-01, 6.0960e+00,\n",
            "         9.6900e+01, 3.7598e+00, 4.0000e+00, 3.0700e+02, 2.1000e+01, 2.4831e+02,\n",
            "         2.0340e+01]]) \n",
            "\n",
            "Conjunto de prueba (X_test_tensor):\n",
            "\n",
            "tensor([[4.3370e-02, 2.1000e+01, 5.6400e+00, 0.0000e+00, 4.3900e-01, 6.1150e+00,\n",
            "         6.3000e+01, 6.8147e+00, 4.0000e+00, 2.4300e+02, 1.6800e+01, 3.9397e+02,\n",
            "         9.4300e+00],\n",
            "        [1.2518e+00, 0.0000e+00, 8.1400e+00, 0.0000e+00, 5.3800e-01, 5.5700e+00,\n",
            "         9.8100e+01, 3.7979e+00, 4.0000e+00, 3.0700e+02, 2.1000e+01, 3.7657e+02,\n",
            "         2.1020e+01],\n",
            "        [8.3080e-02, 0.0000e+00, 2.4600e+00, 0.0000e+00, 4.8800e-01, 5.6040e+00,\n",
            "         8.9800e+01, 2.9879e+00, 3.0000e+00, 1.9300e+02, 1.7800e+01, 3.9100e+02,\n",
            "         1.3980e+01],\n",
            "        [1.3058e-01, 0.0000e+00, 1.0010e+01, 0.0000e+00, 5.4700e-01, 5.8720e+00,\n",
            "         7.3100e+01, 2.4775e+00, 6.0000e+00, 4.3200e+02, 1.7800e+01, 3.3863e+02,\n",
            "         1.5370e+01],\n",
            "        [1.9539e-01, 0.0000e+00, 1.0810e+01, 0.0000e+00, 4.1300e-01, 6.2450e+00,\n",
            "         6.2000e+00, 5.2873e+00, 4.0000e+00, 3.0500e+02, 1.9200e+01, 3.7717e+02,\n",
            "         7.5400e+00]]) \n",
            "\n",
            "Etiquetas de entrenamiento (y_train_tensor):\n",
            "\n",
            "tensor([[35.4000],\n",
            "        [46.0000],\n",
            "        [20.1000],\n",
            "        [21.7000],\n",
            "        [13.5000]]) \n",
            "\n",
            "Etiquetas de prueba (y_test_tensor):\n",
            "\n",
            "tensor([[20.5000],\n",
            "        [13.6000],\n",
            "        [26.4000],\n",
            "        [20.4000],\n",
            "        [23.4000]]) \n",
            "\n",
            "Conjunto de entrenamiento escalado (X_train_tensor):\n",
            "\n",
            "tensor([[-0.4089,  0.3531, -1.1385, -0.2722, -0.9584,  0.9532, -1.0847,  0.6881,\n",
            "         -0.5272, -1.1288, -1.6279,  0.3889, -1.1160],\n",
            "        [-0.4062,  0.3531, -1.1385,  3.6648, -0.9584,  1.9061, -0.6426,  0.6723,\n",
            "         -0.5272, -1.1288, -1.6279,  0.2225, -1.3401],\n",
            "        [ 1.0367, -0.4868,  1.0084, -0.2722,  0.2119, -0.8133, -0.3950, -0.4743,\n",
            "          1.6342,  1.5109,  0.8203,  0.4401,  0.3264],\n",
            "        [-0.3937, -0.4868, -0.0832,  3.6648, -0.5649, -0.4656,  0.8573,  0.0315,\n",
            "         -0.6410, -0.7710,  0.0812,  0.4001,  0.6824],\n",
            "        [-0.2342, -0.4868, -0.4393, -0.2722, -0.1466, -0.2742,  1.0271, -0.0249,\n",
            "         -0.6410, -0.5950,  1.1898, -1.1907,  1.1178]]) \n",
            "\n",
            "Conjunto de prueba escalado (X_test_tensor):\n",
            "\n",
            "tensor([[-0.4082,  0.3951, -0.8027, -0.2722, -0.9917, -0.2474, -0.1721,  1.4418,\n",
            "         -0.6410, -0.9704, -0.7502,  0.4080, -0.4296],\n",
            "        [-0.2742, -0.4868, -0.4393, -0.2722, -0.1466, -1.0145,  1.0695, -0.0066,\n",
            "         -0.6410, -0.5950,  1.1898,  0.2170,  1.2142],\n",
            "        [-0.4038, -0.4868, -1.2649, -0.2722, -0.5734, -0.9667,  0.7759, -0.3955,\n",
            "         -0.7547, -1.2637, -0.2883,  0.3754,  0.2157],\n",
            "        [-0.3986, -0.4868, -0.1675, -0.2722, -0.0698, -0.5895,  0.1852, -0.6405,\n",
            "         -0.4135,  0.1383, -0.2883, -0.1994,  0.4129],\n",
            "        [-0.3914, -0.4868, -0.0512, -0.2722, -1.2136, -0.0644, -2.1813,  0.7085,\n",
            "         -0.6410, -0.6067,  0.3584,  0.2236, -0.6976]]) \n",
            "\n"
          ]
        }
      ]
    },
    {
      "id": "efc809e5",
      "cell_type": "markdown",
      "source": [
        "\n",
        "Aquí definimos una red neuronal con dos capas ocultas para predecir el precio de las casas. Utilizamos funciones de activación ReLU para las capas ocultas.\n"
      ],
      "metadata": {
        "id": "efc809e5"
      }
    },
    {
      "id": "2eeb110f",
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2eeb110f",
        "outputId": "6b0f6c70-0ced-43fa-d6f5-1fd44b5228ef"
      },
      "execution_count": null,
      "source": [
        "class HousePricePredictor(nn.Module):\n",
        "    \"\"\"\n",
        "    Red neuronal multicapa para predecir precios de casas, con tres capas\n",
        "    totalmente conectadas y funciones de activación ReLU en las dos primeras capas.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim):\n",
        "        \"\"\"\n",
        "        Inicializa las capas completamente conectadas de la red neuronal.\n",
        "        \"\"\"\n",
        "        super(HousePricePredictor, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 64)\n",
        "        self.fc2 = nn.Linear(64, 32)\n",
        "        self.fc3 = nn.Linear(32, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Se define el paso hacia adelante de la red, aplicando ReLU a las dos primeras capas.\n",
        "        \"\"\"\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "input_dim = X_train_tensor.shape[1]\n",
        "model = HousePricePredictor(input_dim)\n",
        "print(f\"Modelo de red neuronal creado: {model}\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modelo de red neuronal creado: HousePricePredictor(\n",
            "  (fc1): Linear(in_features=13, out_features=64, bias=True)\n",
            "  (fc2): Linear(in_features=64, out_features=32, bias=True)\n",
            "  (fc3): Linear(in_features=32, out_features=1, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "id": "bde35d6b",
      "cell_type": "markdown",
      "source": [
        "\n",
        "Definimos la función de pérdida utilizando el error cuadrático medio (MSE) y usamos el optimizador Adam para actualizar los pesos de la red durante el entrenamiento.\n"
      ],
      "metadata": {
        "id": "bde35d6b"
      }
    },
    {
      "id": "a3b9dfee",
      "cell_type": "code",
      "metadata": {
        "id": "a3b9dfee"
      },
      "execution_count": null,
      "source": [
        "# Función de pérdida\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n"
      ],
      "outputs": []
    },
    {
      "id": "c0172908",
      "cell_type": "markdown",
      "source": [
        "\n",
        "Entrenamos el modelo utilizando el conjunto de datos de entrenamiento. La pérdida se imprime cada 10 épocas para monitorizar el progreso.\n"
      ],
      "metadata": {
        "id": "c0172908"
      }
    },
    {
      "id": "ac558a62",
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ac558a62",
        "outputId": "9a03a57d-00a3-4257-a829-53f8855c85ac"
      },
      "execution_count": null,
      "source": [
        "\n",
        "def train_model(model, criterion, optimizer, X_train, y_train, epochs=100):\n",
        "    \"\"\"\n",
        "    Entrena el modelo durante un número específico de épocas utilizando el conjunto de datos de entrenamiento.\n",
        "    \"\"\"\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_train)\n",
        "        loss = criterion(outputs, y_train)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")\n",
        "\n",
        "train_model(model, criterion, optimizer, X_train_tensor, y_train_tensor)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100, Loss: 597.7412109375\n",
            "Epoch 2/100, Loss: 585.080810546875\n",
            "Epoch 3/100, Loss: 570.7589721679688\n",
            "Epoch 4/100, Loss: 552.3079223632812\n",
            "Epoch 5/100, Loss: 528.4374389648438\n",
            "Epoch 6/100, Loss: 498.5988464355469\n",
            "Epoch 7/100, Loss: 462.3499755859375\n",
            "Epoch 8/100, Loss: 419.6030578613281\n",
            "Epoch 9/100, Loss: 370.7037048339844\n",
            "Epoch 10/100, Loss: 316.71270751953125\n",
            "Epoch 11/100, Loss: 259.7393493652344\n",
            "Epoch 12/100, Loss: 203.23452758789062\n",
            "Epoch 13/100, Loss: 152.25320434570312\n",
            "Epoch 14/100, Loss: 113.53994750976562\n",
            "Epoch 15/100, Loss: 94.09928894042969\n",
            "Epoch 16/100, Loss: 97.07152557373047\n",
            "Epoch 17/100, Loss: 115.6932373046875\n",
            "Epoch 18/100, Loss: 133.18214416503906\n",
            "Epoch 19/100, Loss: 135.74977111816406\n",
            "Epoch 20/100, Loss: 122.01611328125\n",
            "Epoch 21/100, Loss: 98.84598541259766\n",
            "Epoch 22/100, Loss: 74.45112609863281\n",
            "Epoch 23/100, Loss: 54.78443145751953\n",
            "Epoch 24/100, Loss: 42.3873405456543\n",
            "Epoch 25/100, Loss: 36.91046905517578\n",
            "Epoch 26/100, Loss: 36.37610626220703\n",
            "Epoch 27/100, Loss: 38.37968444824219\n",
            "Epoch 28/100, Loss: 40.83677291870117\n",
            "Epoch 29/100, Loss: 42.39401626586914\n",
            "Epoch 30/100, Loss: 42.405845642089844\n",
            "Epoch 31/100, Loss: 40.833438873291016\n",
            "Epoch 32/100, Loss: 38.05476379394531\n",
            "Epoch 33/100, Loss: 34.695377349853516\n",
            "Epoch 34/100, Loss: 31.48980712890625\n",
            "Epoch 35/100, Loss: 29.07245445251465\n",
            "Epoch 36/100, Loss: 27.81789207458496\n",
            "Epoch 37/100, Loss: 27.724197387695312\n",
            "Epoch 38/100, Loss: 28.377147674560547\n",
            "Epoch 39/100, Loss: 29.101608276367188\n",
            "Epoch 40/100, Loss: 29.234384536743164\n",
            "Epoch 41/100, Loss: 28.4150333404541\n",
            "Epoch 42/100, Loss: 26.690155029296875\n",
            "Epoch 43/100, Loss: 24.436193466186523\n",
            "Epoch 44/100, Loss: 22.168481826782227\n",
            "Epoch 45/100, Loss: 20.319372177124023\n",
            "Epoch 46/100, Loss: 19.10124969482422\n",
            "Epoch 47/100, Loss: 18.503990173339844\n",
            "Epoch 48/100, Loss: 18.345876693725586\n",
            "Epoch 49/100, Loss: 18.374671936035156\n",
            "Epoch 50/100, Loss: 18.36980628967285\n",
            "Epoch 51/100, Loss: 18.198787689208984\n",
            "Epoch 52/100, Loss: 17.828765869140625\n",
            "Epoch 53/100, Loss: 17.321258544921875\n",
            "Epoch 54/100, Loss: 16.786270141601562\n",
            "Epoch 55/100, Loss: 16.334564208984375\n",
            "Epoch 56/100, Loss: 16.0350341796875\n",
            "Epoch 57/100, Loss: 15.887518882751465\n",
            "Epoch 58/100, Loss: 15.830799102783203\n",
            "Epoch 59/100, Loss: 15.769464492797852\n",
            "Epoch 60/100, Loss: 15.62378978729248\n",
            "Epoch 61/100, Loss: 15.360681533813477\n",
            "Epoch 62/100, Loss: 15.000345230102539\n",
            "Epoch 63/100, Loss: 14.60335636138916\n",
            "Epoch 64/100, Loss: 14.233189582824707\n",
            "Epoch 65/100, Loss: 13.934806823730469\n",
            "Epoch 66/100, Loss: 13.714534759521484\n",
            "Epoch 67/100, Loss: 13.555329322814941\n",
            "Epoch 68/100, Loss: 13.423391342163086\n",
            "Epoch 69/100, Loss: 13.290738105773926\n",
            "Epoch 70/100, Loss: 13.14057445526123\n",
            "Epoch 71/100, Loss: 12.973607063293457\n",
            "Epoch 72/100, Loss: 12.801803588867188\n",
            "Epoch 73/100, Loss: 12.642385482788086\n",
            "Epoch 74/100, Loss: 12.50558090209961\n",
            "Epoch 75/100, Loss: 12.389726638793945\n",
            "Epoch 76/100, Loss: 12.28492259979248\n",
            "Epoch 77/100, Loss: 12.17863941192627\n",
            "Epoch 78/100, Loss: 12.061552047729492\n",
            "Epoch 79/100, Loss: 11.934273719787598\n",
            "Epoch 80/100, Loss: 11.802675247192383\n",
            "Epoch 81/100, Loss: 11.674728393554688\n",
            "Epoch 82/100, Loss: 11.55583381652832\n",
            "Epoch 83/100, Loss: 11.449211120605469\n",
            "Epoch 84/100, Loss: 11.351522445678711\n",
            "Epoch 85/100, Loss: 11.259779930114746\n",
            "Epoch 86/100, Loss: 11.170570373535156\n",
            "Epoch 87/100, Loss: 11.082361221313477\n",
            "Epoch 88/100, Loss: 10.996618270874023\n",
            "Epoch 89/100, Loss: 10.914718627929688\n",
            "Epoch 90/100, Loss: 10.840591430664062\n",
            "Epoch 91/100, Loss: 10.773347854614258\n",
            "Epoch 92/100, Loss: 10.708769798278809\n",
            "Epoch 93/100, Loss: 10.64356803894043\n",
            "Epoch 94/100, Loss: 10.576262474060059\n",
            "Epoch 95/100, Loss: 10.507462501525879\n",
            "Epoch 96/100, Loss: 10.43923568725586\n",
            "Epoch 97/100, Loss: 10.374603271484375\n",
            "Epoch 98/100, Loss: 10.313979148864746\n",
            "Epoch 99/100, Loss: 10.257238388061523\n",
            "Epoch 100/100, Loss: 10.202234268188477\n"
          ]
        }
      ]
    },
    {
      "id": "14768318",
      "cell_type": "markdown",
      "source": [
        "\n",
        "Evaluamos el rendimiento del modelo en el conjunto de datos de prueba y calculamos el error cuadrático medio.\n"
      ],
      "metadata": {
        "id": "14768318"
      }
    },
    {
      "id": "cd7a01bd",
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cd7a01bd",
        "outputId": "221afa25-a745-41f9-c82d-66debaf77eab"
      },
      "execution_count": null,
      "source": [
        "\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    \"\"\"\n",
        "    Evalúa el rendimiento del modelo en el conjunto de datos de prueba y calcula el error cuadrático medio.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        predictions = model(X_test)\n",
        "        loss = criterion(predictions, y_test)\n",
        "    print(f\"Error Cuadrático Medio en el conjunto de prueba: {loss.item()}\")\n",
        "\n",
        "evaluate_model(model, X_test_tensor, y_test_tensor)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error Cuadrático Medio en el conjunto de prueba: 22.647968292236328\n"
          ]
        }
      ]
    },
    {
      "id": "773b12da",
      "cell_type": "markdown",
      "source": [
        "\n",
        "Simulamos las características de una nueva casa, las escalamos utilizando el mismo escalador que se usó en los datos de entrenamiento, y luego predecimos el precio de la casa utilizando el modelo entrenado.\n"
      ],
      "metadata": {
        "id": "773b12da"
      }
    },
    {
      "id": "9f0e4a7a",
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9f0e4a7a",
        "outputId": "ac897afd-dc28-41bc-9e3f-ffa47af544d2"
      },
      "execution_count": null,
      "source": [
        "mean = X_train_tensor.mean(dim=0, keepdim=True)\n",
        "std = X_train_tensor.std(dim=0, keepdim=True)\n",
        "\n",
        "new_house = torch.tensor([[0.00632, 18.0, 2.31, 0.0, 0.538, 6.575, 65.2, 4.0900, 1.0, 296.0, 15.3, 396.90, 4.98]], dtype=torch.float32)\n",
        "\n",
        "new_house_scaled = (new_house - mean) / std\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    predicted_price = model(new_house_scaled)\n",
        "print(f\"Predicción del precio de la nueva casa: ${predicted_price.item() * 1000:.2f}\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicción del precio de la nueva casa: $1643290.41\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Índice | Característica | Valor  | Descripción                                                                 | Evaluación          |\n",
        "|--------|----------------|--------|-----------------------------------------------------------------------------|---------------------|\n",
        "| 0      | CRIM           | 0.00632 | Tasa de criminalidad per cápita por localidad.                               | Favorable (bajo)    |\n",
        "| 1      | ZN             | 18.0    | Proporción de terreno residencial dividido en zonas para lotes de más de 25,000 pies cuadrados. | Favorable           |\n",
        "| 2      | INDUS          | 2.31    | Proporción de acres de negocios no minoristas por ciudad.                    | Favorable (bajo)    |\n",
        "| 3      | CHAS           | 0.0     | Variable ficticia del Río Charles (1 si limita con el río; 0 de lo contrario).| No favorable        |\n",
        "| 4      | NOX            | 0.538   | Concentración de óxidos nítricos (partes por 10 millones).                   | Favorable (bajo)    |\n",
        "| 5      | RM             | 6.575   | Número promedio de habitaciones por vivienda.                                | Favorable           |\n",
        "| 6      | AGE            | 65.2    | Proporción de unidades ocupadas por propietarios construidas antes de 1940.  | No favorable (alto) |\n",
        "| 7      | DIS            | 4.0900  | Distancias ponderadas a cinco centros de empleo en Boston.                   | Favorable           |\n",
        "| 8      | RAD            | 1.0     | Índice de accesibilidad a carreteras radiales.                               | Favorable           |\n",
        "| 9      | TAX            | 296.0   | Tasa de impuesto a la propiedad por cada $10,000.                            | Favorable (bajo)    |\n",
        "| 10     | PTRATIO        | 15.3    | Proporción alumno-maestro por ciudad.                                        | Favorable (bajo)    |\n",
        "| 11     | B              | 396.90  | 1000(Bk - 0.63)^2 donde Bk es la proporción de negros por ciudad.            | Favorable           |\n",
        "| 12     | LSTAT          | 4.98    | Porcentaje de estatus inferior de la población.                              | Favorable (bajo)    |\n"
      ],
      "metadata": {
        "id": "TR1ryplV080l"
      },
      "id": "TR1ryplV080l"
    }
  ]
}